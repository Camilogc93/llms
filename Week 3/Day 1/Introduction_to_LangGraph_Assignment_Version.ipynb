{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "\n",
        "  - ü§ù Breakout Room #2:\n",
        "  1. Evaluating the LangGraph Application with LangSmith\n",
        "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
        "  3. LangGraph for the \"Patterns\" of GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KaVwN269EttM"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain_openai langchain-community langgraph arxiv duckduckgo_search==5.3.1b1\n",
        "#!conda install -n agents ipykernel --update-deps --force-reinstall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "42b2ba5e-11ae-4f4d-e68e-77607bb794ad"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "30aa2260-50f1-4abf-b305-eed0ee1b9008"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE4 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun(),\n",
        "    ArxivQueryRun()\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'They show Dettmers explaining to EleutherAI researchers that Meta lawyers had concerns with using The Pile, an AI training dataset created by EleutherAI that includes Books3. The plaintiffs\\' attorneys have since redacted the quotes from the amended complaint, but screenshots of the chats have continued to circulate on LinkedIn, according to ... In the chat quoted in the complaint, researcher Tim Dettmers talks about his back-and-forth with Meta\\'s legal department whether the use of the book files as training data would be \"legally ok ... In the chat logs quoted in the complaint, researcher Tim Dettmers describes his back-and-forth with Meta\\'s legal department over whether use of the book files as training data would be \"legally ok.\" The month prior, Dettmers wrote that Meta\\'s lawyers had told him \"the data cannot be used or models cannot be published if they are trained on that data,\" the complaint said. While Dettmers does not describe the lawyers\\' concerns, his counterparts in the chat identify \"books with active copyrights\" as the biggest likely source of worry. The month prior, Dettmers wrote that Meta\\'s lawyers had told him \"the data cannot be used or models cannot be published if they are trained on that data,\" the complaint said. While Dettmers does not describe the lawyers\\' concerns, his counterparts in the chat identify \"books with active copyrights\" as the biggest likely source of worry.'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#check_tool\n",
        "search = DuckDuckGoSearchRun()\n",
        "\n",
        "search.invoke(\"Tim Dettmers latest Tweet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "model = model.bind_tools(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "Every function or tool we create must have a clear explanation of its usage and purpose; this is how the model decides whether to use it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "tool_node = ToolNode(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `tool_node` is a node which can call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "uncompiled_graph = StateGraph(AgentState)\n",
        "\n",
        "uncompiled_graph.add_node(\"agent\", call_model)\n",
        "uncompiled_graph.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "uncompiled_graph.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  return END\n",
        "\n",
        "uncompiled_graph.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "uncompiled_graph.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "compiled_graph = uncompiled_graph.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "I can't find any useful information on this.\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?\n",
        "\n",
        "Yes,we can use a conditional tool to count the iterations and determine if it's sufficient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "a5d7ef7a-13f2-4066-df52-b0df89eae2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_z2VDOK2czcSlhEDmYm0ZcgzW', 'function': {'arguments': '{\"query\":\"PETF util LLMs\"}', 'name': 'arxiv'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 157, 'total_tokens': 176}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-1a66a27c-fc9f-47e6-adb5-44790dc09511-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'PETF util LLMs'}, 'id': 'call_z2VDOK2czcSlhEDmYm0ZcgzW', 'type': 'tool_call'}], usage_metadata={'input_tokens': 157, 'output_tokens': 19, 'total_tokens': 176})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content=\"Published: 2024-06-13\\nTitle: Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications\\nAuthors: Irene Weber\\nSummary: Large Language Models (LLMs) have become widely adopted recently. Research\\nexplores their use both as autonomous agents and as tools for software\\nengineering. LLM-integrated applications, on the other hand, are software\\nsystems that leverage an LLM to perform tasks that would otherwise be\\nimpossible or require significant coding effort. While LLM-integrated\\napplication engineering is emerging as new discipline, its terminology,\\nconcepts and methods need to be established. This study provides a taxonomy for\\nLLM-integrated applications, offering a framework for analyzing and describing\\nthese systems. It also demonstrates various ways to utilize LLMs in\\napplications, as well as options for implementing such integrations.\\n  Following established methods, we analyze a sample of recent LLM-integrated\\napplications to identify relevant dimensions. We evaluate the taxonomy by\\napplying it to additional cases. This review shows that applications integrate\\nLLMs in numerous ways for various purposes. Frequently, they comprise multiple\\nLLM integrations, which we term ``LLM components''. To gain a clear\\nunderstanding of an application's architecture, we examine each LLM component\\nseparately. We identify thirteen dimensions along which to characterize an LLM\\ncomponent, including the LLM skills leveraged, the format of the output, and\\nmore. LLM-integrated applications are described as combinations of their LLM\\ncomponents. We suggest a concise representation using feature vectors for\\nvisualization.\\n  The taxonomy is effective for describing LLM-integrated applications. It can\\ncontribute to theory building in the nascent field of LLM-integrated\\napplication engineering and aid in developing such systems. Researchers and\\npractitioners explore numerous creative ways to leverage LLMs in applications.\\nThough challenges persist, integrating LLMs may revolutionize the way software\\nsystems are built.\\n\\nPublished: 2023-11-09\\nTitle: Combating Misinformation in the Age of LLMs: Opportunities and Challenges\\nAuthors: Canyu Chen, Kai Shu\\nSummary: Misinformation such as fake news and rumors is a serious threat on\\ninformation ecosystems and public trust. The emergence of Large Language Models\\n(LLMs) has great potential to reshape the landscape of combating\\nmisinformation. Generally, LLMs can be a double-edged sword in the fight. On\\nthe one hand, LLMs bring promising opportunities for combating misinformation\\ndue to their profound world knowledge and strong reasoning abilities. Thus, one\\nemergent question is: how to utilize LLMs to combat misinformation? On the\\nother hand, the critical challenge is that LLMs can be easily leveraged to\\ngenerate deceptive misinformation at scale. Then, another important question\\nis: how to combat LLM-generated misinformation? In this paper, we first\\nsystematically review the history of combating misinformation before the advent\\nof LLMs. Then we illustrate the current efforts and present an outlook for\\nthese two fundamental questions respectively. The goal of this survey paper is\\nto facilitate the progress of utilizing LLMs for fighting misinformation and\\ncall for interdisciplinary efforts from different stakeholders for combating\\nLLM-generated misinformation.\\n\\nPublished: 2024-08-05\\nTitle: LLM economicus? Mapping the Behavioral Biases of LLMs via Utility Theory\\nAuthors: Jillian Ross, Yoon Kim, Andrew W. Lo\\nSummary: Humans are not homo economicus (i.e., rational economic beings). As humans,\\nwe exhibit systematic behavioral biases such as loss aversion, anchoring,\\nframing, etc., which lead us to make suboptimal economic decisions. Insofar as\\nsuch biases may be embedded in text data on which large language models (LLMs)\\nare trained, to what extent are LLMs prone to the same behavioral biases?\\nUnderstanding these biases in LLMs is crucial for deploying LLMs to support\\nhuman decision-making. \", name='arxiv', tool_call_id='call_z2VDOK2czcSlhEDmYm0ZcgzW')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='The term \"PETF\" in the context of utilizing Large Language Models (LLMs) does not appear explicitly in the retrieved articles. However, the articles provide insights into various aspects of LLMs and their applications, which might help infer the utility of PETF when trying LLMs.\\n\\n1. **Taxonomy for LLM-Integrated Applications**:\\n   - LLMs are increasingly used as software components in various applications, performing tasks that would otherwise require significant coding effort.\\n   - A taxonomy for LLM-integrated applications helps in analyzing and describing these systems, identifying relevant dimensions such as LLM skills leveraged, output format, and more.\\n   - This structured approach aids in understanding the architecture of applications that integrate LLMs, potentially making the development and deployment of such systems more efficient.\\n\\n2. **Combating Misinformation**:\\n   - LLMs have the potential to combat misinformation due to their profound world knowledge and strong reasoning abilities.\\n   - However, they can also generate deceptive misinformation at scale, posing a challenge.\\n   - Understanding how to utilize LLMs effectively in this context is crucial for maintaining information integrity.\\n\\n3. **Behavioral Biases in LLMs**:\\n   - LLMs, trained on text data that may contain human biases, can exhibit similar behavioral biases.\\n   - Understanding these biases is important for deploying LLMs in decision-making processes to ensure they support rather than hinder optimal outcomes.\\n\\nWhile the specific term \"PETF\" is not addressed, the utility of LLMs in various applications is evident. They can significantly enhance software capabilities, combat misinformation, and support decision-making, provided their biases are understood and managed. If \"PETF\" refers to a specific framework or methodology, further details would be needed to provide a precise explanation.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 363, 'prompt_tokens': 1014, 'total_tokens': 1377}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None}, id='run-8ccdad32-8ef7-418b-94b2-313c1769250b-0', usage_metadata={'input_tokens': 1014, 'output_tokens': 363, 'total_tokens': 1377})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"Why is PETF util when trying LLMs?\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "026a3aa3-1c3e-4016-b0a3-fe98b1d25399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_SnMwOH26T7g9K2SE39k9w0st', 'function': {'arguments': '{\"query\": \"QLoRA\"}', 'name': 'arxiv'}, 'type': 'function'}, {'id': 'call_C8IdEzpBj3N7lbqLAKyiUbOr', 'function': {'arguments': '{\"query\": \"latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 173, 'total_tokens': 223}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-3af96c1e-3eee-4c2c-8810-1ac9d22b3d9f-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'QLoRA'}, 'id': 'call_SnMwOH26T7g9K2SE39k9w0st', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'latest Tweet'}, 'id': 'call_C8IdEzpBj3N7lbqLAKyiUbOr', 'type': 'tool_call'}], usage_metadata={'input_tokens': 173, 'output_tokens': 50, 'total_tokens': 223})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: arxiv\n",
            "[ToolMessage(content='Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2024-06-12\\nTitle: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\\nAuthors: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\\nSummary: There are various methods for adapting LLMs to different domains. The most\\ncommon methods are prompting, finetuning, and RAG. In this w', name='arxiv', tool_call_id='call_SnMwOH26T7g9K2SE39k9w0st'), ToolMessage(content=\"NEW YORK (AP) ‚Äî Donald Trump shared more than a dozen posts on his social media network Wednesday that call for the trial or jailing of House lawmakers who investigated the attack on the U.S. Capitol, special counsel Jack Smith and others, along with images that reference the QAnon conspiracy theory. The former president began posting a string of messages Tuesday evening after Smith filed a ... The latest Twitter news and updates. Twitter is a social networking service, primarily microblogging but also a picture and video sharing service, founded by Jack Dorsey, Noah Glass, Biz Stone and ... Twitter adds 'glorifying violence' warning to Trump tweet. Twitter has added a warning to one of President Donald Trump's tweets about protests in Minneapolis. ... In his latest broadside ... On Twitter, people are getting ready for it in the most Twitter way possible. Here are the best pre-debate tweets that helped us laugh through our nerves: ... new memes today pic.twitter.com ... In more serious matters, EU Commissioner Thierry Breton visited Twitter's headquarters to conduct a stress test for the new Digital Services Act regulating everything from social media content ...\", name='duckduckgo_search', tool_call_id='call_C8IdEzpBj3N7lbqLAKyiUbOr')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Nzcw8eA9Y2pKi9kKZZeqZtHd', 'function': {'arguments': '{\"query\": \"Tim Dettmers latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_tFL4nmCjfHZunhYYUP5HHX4k', 'function': {'arguments': '{\"query\": \"Artidoro Pagnoni latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_jfWQaoLuFTYNmLuOALMv6Otd', 'function': {'arguments': '{\"query\": \"Ari Holtzman latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_fq57uPjdqBwIq8guWelgMprI', 'function': {'arguments': '{\"query\": \"Luke Zettlemoyer latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 108, 'prompt_tokens': 1429, 'total_tokens': 1537}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-dd91290f-0f45-48f6-9e69-f135769ebf28-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'Tim Dettmers latest Tweet'}, 'id': 'call_Nzcw8eA9Y2pKi9kKZZeqZtHd', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Artidoro Pagnoni latest Tweet'}, 'id': 'call_tFL4nmCjfHZunhYYUP5HHX4k', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Ari Holtzman latest Tweet'}, 'id': 'call_jfWQaoLuFTYNmLuOALMv6Otd', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Luke Zettlemoyer latest Tweet'}, 'id': 'call_fq57uPjdqBwIq8guWelgMprI', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1429, 'output_tokens': 108, 'total_tokens': 1537})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: duckduckgo_search\n",
            "[ToolMessage(content='They show Dettmers explaining to EleutherAI researchers that Meta lawyers had concerns with using The Pile, an AI training dataset created by EleutherAI that includes Books3. The plaintiffs\\' attorneys have since redacted the quotes from the amended complaint, but screenshots of the chats have continued to circulate on LinkedIn, according to ... In the chat quoted in the complaint, researcher Tim Dettmers talks about his back-and-forth with Meta\\'s legal department whether the use of the book files as training data would be \"legally ok ... In the chat logs quoted in the complaint, researcher Tim Dettmers describes his back-and-forth with Meta\\'s legal department over whether use of the book files as training data would be \"legally ok.\" Dettmers has also accepted a position as a professor with Carnegie Mellon University, where he will be starting in the fall of next year. Ai2 is led by CEO Ali Farhadi , who is also a UW Allen ... The month prior, Dettmers wrote that Meta\\'s lawyers had told him \"the data cannot be used or models cannot be published if they are trained on that data,\" the complaint said. While Dettmers does not describe the lawyers\\' concerns, his counterparts in the chat identify \"books with active copyrights\" as the biggest likely source of worry.', name='duckduckgo_search', tool_call_id='call_Nzcw8eA9Y2pKi9kKZZeqZtHd'), ToolMessage(content=\"Saturday, June 29, 2024. Introduction. Fine-tuning large language models (LLMs) is a common practice to adapt them for specific tasks, but it can be computationally expensive. LoRA (Low-Rank Adaptation) is a technique that makes this process more efficient by introducing small adapter modules to the model. These adapters capture task-specific ... Post-Training Quantization (PTQ) enhances the efficiency of Large Language Models (LLMs) by enabling faster operation and compatibility with more accessible hardware through reduced memory usage, at the cost of small performance drops. We explore the role of calibration sets in PTQ, specifically their effect on hidden activations in various ... efficient finetuning of quantized LLMs. AUTHORs: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer Authors Info & Claims. NIPS '23: Proceedings of the 37th International Conference on Neural Information Processing Systems. Article No.: 441, Pages 10088 - 10115. Published: 30 May 2024 Publication History. Artidoro Pagnoni. artidoro. Follow. mrm8488's profile picture Weyaxi's profile picture nezubn's profile picture. ... artidoro/model-tvergho. Updated Nov 18, 2023. artidoro/model-vinaic. Updated Nov 18, 2023. artidoro/model-vinaia. Updated Nov 18, 2023. datasets. None public yet. Company In this paper, we address these aforementioned challenges associated with financial data and introduce FinGPT, an end-to-end open-source framework for financial large language models (FinLLMs). Adopting a data-centric approach, FinGPT underscores the crucial role of data acquisition, cleaning, and preprocessing in developing open-source FinLLMs.\", name='duckduckgo_search', tool_call_id='call_tFL4nmCjfHZunhYYUP5HHX4k'), ToolMessage(content=\"Ari Lennox is worried about her safety. Ari Lennox says she's worried about exposing the people who broke into her home in Los Angeles because she doesn't want them to come back for her again. The team behind QLoRA includes Allen School Ph.D. student Artidoro Pagnoni; alum Ari Holtzman (Ph.D., '23), incoming professor at the University of Chicago; and professor Luke Zettlemoyer, who is also a research manager at Meta. Madrona Prize First Runner Up / Punica: Multi-Tenant LoRA Fine-tuned LLM Serving Humans rate the disinformation generated by Grover as trustworthy, even more so than human-written disinformation. Thus, developing robust verification techniques against generators such as Grover is an important research area. We consider a setting in which a discriminator has access to 5000 Grover generations, but unlimited access to real news. . In this setting, the best existing fake news ... Ari Holtzman, Assistant Professor of Computer and Data Science Teaching: CMSC 14100-5: Introduction to Computer Science I, CMSC 35900-1: Topics in Artificial Intelligence ... Holtzman is excited to bring his innovative approaches to UChicago, where he plans to engage with students and colleagues alike. ... Link to Twitter Link to Linkedin Link ... Aria Holtzman '27 surprised her grandfather, Shaojing Zhang, the conductor of the Shenyang Philharmonic, by walking on stage while he was filming a TV show in China celebrating the Lunar New Year. Holtzman said her grandfather, who she hadn't seen in person for five years, paused for a moment when he was told they had a surprise for him.\", name='duckduckgo_search', tool_call_id='call_jfWQaoLuFTYNmLuOALMv6Otd'), ToolMessage(content=\"Luke Zettlemoyer is a research manager and site lead for FAIR Seattle. He is also a Professor in the Allen School of Computer Science & Engineering at the University of Washington. His research is in empirical computational semantics, where the goal is to build models that recover representations of the meaning of natural language text. We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data. Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences. We pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws ... Today we're joined by Luke Zettlemoyer, professor at University of Washington and a research manager at Meta. In our conversation with Luke, we cover multimodal generative AI, the effect of data on models, and the significance of open source and open science. We explore the grounding problem, the need for visual grounding and embodiment in \\ufeff Twitter \\ufeff Reddit. Join our list for notifications and early access to events ... About this Episode. Today we're joined by Luke Zettlemoyer, professor at University of Washington and a research manager at Meta. In our conversation with Luke, we cover multimodal generative AI, the effect of data on models, and the significance of open ... Evaluating the factuality of long-form text generated by large language models (LMs) is nontrivial because (1) generations often contain a mixture of...\", name='duckduckgo_search', tool_call_id='call_fq57uPjdqBwIq8guWelgMprI')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content=\"Here are the latest updates related to the authors of the QLoRA paper:\\n\\n1. **Tim Dettmers**:\\n   - Tim Dettmers has been involved in discussions with Meta's legal department regarding the use of book files as training data. He has also accepted a position as a professor at Carnegie Mellon University, starting in the fall of next year.\\n\\n2. **Artidoro Pagnoni**:\\n   - Artidoro Pagnoni is actively involved in research and has been updating models such as `model-tvergho`, `model-vinaic`, and `model-vinaia` as of November 2023. He is also part of the team behind QLoRA.\\n\\n3. **Ari Holtzman**:\\n   - Ari Holtzman is an incoming professor at the University of Chicago and has been involved in various research projects, including the development of robust verification techniques against disinformation generators like Grover.\\n\\n4. **Luke Zettlemoyer**:\\n   - Luke Zettlemoyer is a research manager and site lead for FAIR Seattle, as well as a professor at the University of Washington. He has been working on multimodal generative AI and the significance of open source and open science.\\n\\nUnfortunately, I couldn't find their latest Tweets directly, but these updates provide a glimpse into their recent activities and contributions.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 275, 'prompt_tokens': 2853, 'total_tokens': 3128}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None}, id='run-4008e171-a458-4065-8195-58aaf05c1f80-0', usage_metadata={'input_tokens': 2853, 'output_tokens': 275, 'total_tokens': 3128})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the QLoRA paper, then search each of the authors to find out their latest Tweet using DuckDuckGo.\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        if node == \"action\":\n",
        "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
        "        print(values[\"messages\"])\n",
        "\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "####üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7c8-Uyarh1v"
      },
      "source": [
        "## Part 1: LangSmith Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | compiled_graph | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "b160f3e4-89d1-4411-ed96-fe17b3cad200"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RAG stands for Retrieval-Augmented Generation. It is a technique used in natural language processing (NLP) and machine learning to improve the performance of language models by combining retrieval-based methods with generative models. Here‚Äôs a brief overview of how it works:\\n\\n1. **Retrieval**: In the first step, the system retrieves relevant documents or pieces of information from a large corpus or database. This is typically done using a retrieval model, such as BM25 or a dense retrieval model like DPR (Dense Passage Retrieval).\\n\\n2. **Augmentation**: The retrieved documents are then used to augment the input to the generative model. This means that the generative model has access to additional context or information that can help it produce more accurate and relevant responses.\\n\\n3. **Generation**: Finally, the generative model, such as GPT-3 or BERT, uses the augmented input to generate a response. The additional context provided by the retrieved documents helps the model generate more informed and contextually appropriate responses.\\n\\nRAG is particularly useful in scenarios where the generative model alone might not have enough information to produce a high-quality response, such as in open-domain question answering, conversational agents, and other applications requiring access to a large knowledge base.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What is the main purpose of LLM distillation?\",\n",
        "    \"What method is commonly used to reduce the size of LLMs during distillation?\",\n",
        "    \"What is the trade-off when distilling a large language model?\",\n",
        "    \"Who benefits the most from LLM distillation?\",\n",
        "    \"What is a student model in the context of LLM distillation?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"compress\", \"efficient\"]},\n",
        "    {\"must_mention\" : [\"knowledge\", \"distillation\"]},\n",
        "    {\"must_mention\" : [\"accuracy\", \"size\"]},\n",
        "    {\"must_mention\" : [\"resource\", \"constrained\"]},\n",
        "    {\"must_mention\" : [\"smaller\", \"model\"]}\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about LLM destilation Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "The evaluations appear to be linked to the relevance and accuracy of the provided keywords.\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "We can add a step to check if the response is related to the same context or topic as the associative questions.\n",
        "\n",
        "Improve the keywords to make them more relevant to the answers.\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "8b98fff1-bd75-4dbe-cadc-a76d8a82577c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - eeeb2fbf' at:\n",
            "https://smith.langchain.com/o/5fe1a158-f713-58b1-b71c-95fb6ac31e83/datasets/ea5d9034-8ee7-453b-a908-def12cc75dfe/compare?selectedSessions=859444c9-a1a4-45d1-a61e-82f5c56d5430\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - 0ba54d74 at:\n",
            "https://smith.langchain.com/o/5fe1a158-f713-58b1-b71c-95fb6ac31e83/datasets/ea5d9034-8ee7-453b-a908-def12cc75dfe\n",
            "[------------------------------------------------->] 5/5"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.must_mention</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0b868b53-17ea-4d3f-a004-8f7e171e3579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.026671</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.953983</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.335078</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.899404</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.115665</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.710846</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11.072361</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       feedback.must_mention error  execution_time  \\\n",
              "count                      5     0        5.000000   \n",
              "unique                     2     0             NaN   \n",
              "top                     True   NaN             NaN   \n",
              "freq                       3   NaN             NaN   \n",
              "mean                     NaN   NaN        6.026671   \n",
              "std                      NaN   NaN        2.953983   \n",
              "min                      NaN   NaN        3.335078   \n",
              "25%                      NaN   NaN        4.899404   \n",
              "50%                      NaN   NaN        5.115665   \n",
              "75%                      NaN   NaN        5.710846   \n",
              "max                      NaN   NaN       11.072361   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      5  \n",
              "unique                                     5  \n",
              "top     0b868b53-17ea-4d3f-a004-8f7e171e3579  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - eeeb2fbf',\n",
              " 'results': {'dcee2b26-8ef2-49a8-86c6-78019a316e55': {'input': {'question': 'What is the main purpose of LLM distillation?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('ce420a9d-bca9-4121-a32b-a2ec0138115b'), target_run_id=None)],\n",
              "   'execution_time': 3.335078,\n",
              "   'run_id': '0b868b53-17ea-4d3f-a004-8f7e171e3579',\n",
              "   'output': 'The main purpose of Large Language Model (LLM) distillation is to transfer the knowledge from a large, complex model (teacher model) to a smaller, more efficient model (student model). This process aims to retain the performance and capabilities of the large model while significantly reducing the computational resources required for inference. The key benefits of LLM distillation include:\\n\\n1. **Efficiency**: The distilled model is smaller and faster, making it more suitable for deployment in resource-constrained environments such as mobile devices or edge computing.\\n\\n2. **Scalability**: Smaller models can be scaled more easily across multiple devices or services, enabling broader accessibility and usage.\\n\\n3. **Cost Reduction**: Reduced computational requirements lead to lower operational costs, including energy consumption and hardware expenses.\\n\\n4. **Latency**: Faster inference times result in lower latency, which is crucial for real-time applications.\\n\\n5. **Accessibility**: Smaller models can be more easily shared and used by a wider audience, including those with limited access to high-performance computing resources.\\n\\nOverall, LLM distillation helps in making powerful language models more practical and accessible for various applications.',\n",
              "   'reference': {'must_mention': ['compress', 'efficient']}},\n",
              "  '5be56fbb-c5be-4b33-963b-11838541d7ed': {'input': {'question': 'What method is commonly used to reduce the size of LLMs during distillation?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('a214e2b0-be9e-490b-ac77-0b6fa58380e3'), target_run_id=None)],\n",
              "   'execution_time': 4.899404,\n",
              "   'run_id': '245ac629-aad2-491c-99d1-83a03a5f05d7',\n",
              "   'output': 'A commonly used method to reduce the size of Large Language Models (LLMs) during distillation is **knowledge distillation**. This process involves training a smaller model (the \"student\") to replicate the behavior of a larger, pre-trained model (the \"teacher\"). The key steps in knowledge distillation typically include:\\n\\n1. **Training the Teacher Model**: A large, complex model is trained on a large dataset to achieve high performance.\\n2. **Generating Soft Targets**: The teacher model is used to generate \"soft targets\" or probability distributions over the output classes for each input in the training set.\\n3. **Training the Student Model**: The student model is trained using a combination of the original training data and the soft targets provided by the teacher model. The loss function often includes a term that measures the difference between the student\\'s predictions and the soft targets, as well as a term that measures the difference between the student\\'s predictions and the true labels.\\n\\nThis method allows the student model to learn not only the final predictions of the teacher model but also the intermediate representations and the \"dark knowledge\" captured in the soft targets, which can lead to better generalization and performance despite the reduced size.\\n\\nOther techniques that can be used in conjunction with knowledge distillation include:\\n\\n- **Quantization**: Reducing the precision of the model\\'s weights and activations.\\n- **Pruning**: Removing less important weights or neurons from the model.\\n- **Parameter Sharing**: Sharing parameters across different parts of the model to reduce redundancy.\\n\\nThese methods help in creating a more efficient and compact model while maintaining a significant portion of the original model\\'s performance.',\n",
              "   'reference': {'must_mention': ['knowledge', 'distillation']}},\n",
              "  'c7b72aa2-b414-41be-bb61-85262028e7cd': {'input': {'question': 'What is the trade-off when distilling a large language model?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('7dcb85a3-38bf-40e7-b4ac-50eba61e51e9'), target_run_id=None)],\n",
              "   'execution_time': 11.072361,\n",
              "   'run_id': 'c7a3f0e2-4727-450b-82bc-4be0585758be',\n",
              "   'output': 'Distilling a large language model involves creating a smaller, more efficient model that approximates the performance of the larger model. This process is known as \"knowledge distillation.\" The trade-offs involved in this process include:\\n\\n1. **Performance vs. Efficiency**:\\n   - **Performance**: The distilled model may not perform as well as the original larger model, especially on complex tasks. There is often a trade-off between the size of the model and its accuracy or performance.\\n   - **Efficiency**: The distilled model is more efficient in terms of computational resources, such as memory and processing power. It can be deployed on devices with limited resources and can perform inference faster.\\n\\n2. **Generalization vs. Specialization**:\\n   - **Generalization**: The larger model may have better generalization capabilities due to its higher capacity and ability to capture more nuanced patterns in the data.\\n   - **Specialization**: The distilled model may be more specialized and optimized for specific tasks or datasets, potentially at the cost of generalization to other tasks or datasets.\\n\\n3. **Training Time vs. Inference Time**:\\n   - **Training Time**: Distilling a model requires additional training time and resources to transfer knowledge from the larger model to the smaller one.\\n   - **Inference Time**: Once distilled, the smaller model typically has faster inference times, making it more suitable for real-time applications.\\n\\n4. **Complexity vs. Simplicity**:\\n   - **Complexity**: The larger model may have a more complex architecture, which can capture intricate relationships in the data.\\n   - **Simplicity**: The distilled model has a simpler architecture, which makes it easier to understand, deploy, and maintain.\\n\\n5. **Robustness vs. Vulnerability**:\\n   - **Robustness**: The larger model may be more robust to adversarial attacks and noise due to its higher capacity.\\n   - **Vulnerability**: The distilled model may be more vulnerable to adversarial attacks and noise, as it has fewer parameters and may not capture all the nuances of the larger model.\\n\\nIn summary, the main trade-off when distilling a large language model is between performance and efficiency. While the distilled model is more efficient and easier to deploy, it may not achieve the same level of performance as the original larger model.',\n",
              "   'reference': {'must_mention': ['accuracy', 'size']}},\n",
              "  '44674585-162b-4c3d-9cb5-05d216b34755': {'input': {'question': 'Who benefits the most from LLM distillation?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('4c569a4c-656b-4bcb-82b8-9bba53964fdb'), target_run_id=None)],\n",
              "   'execution_time': 5.115665,\n",
              "   'run_id': '04ec3f6f-9307-4d52-99fe-ec087aa1a135',\n",
              "   'output': 'Large Language Model (LLM) distillation is a process where a smaller model (the \"student\") is trained to replicate the behavior of a larger, more complex model (the \"teacher\"). This process can benefit several groups:\\n\\n1. **Organizations with Limited Computational Resources**:\\n   - Smaller models require less computational power and memory, making them more accessible to organizations that do not have the resources to deploy large models.\\n\\n2. **Edge Computing and IoT Applications**:\\n   - Distilled models can be deployed on edge devices with limited computational capabilities, enabling advanced AI functionalities in environments where deploying large models would be impractical.\\n\\n3. **Developers and Researchers**:\\n   - They can experiment and iterate more quickly with smaller models, reducing the time and cost associated with training and deploying large models.\\n\\n4. **End Users**:\\n   - Users benefit from faster and more efficient AI applications, as smaller models can provide quicker responses and consume less power, which is particularly important for mobile and embedded applications.\\n\\n5. **Environmental Impact**:\\n   - Reducing the size of models can lead to lower energy consumption, which is beneficial for reducing the carbon footprint associated with training and deploying large AI models.\\n\\n6. **Businesses**:\\n   - Companies can reduce operational costs by deploying smaller models that require less infrastructure and maintenance while still delivering high-quality AI services.\\n\\nOverall, LLM distillation democratizes access to advanced AI capabilities, making it possible for a wider range of applications and users to benefit from sophisticated language models.',\n",
              "   'reference': {'must_mention': ['resource', 'constrained']}},\n",
              "  'aedcbd15-de1b-4e10-9dd4-92e11023e12d': {'input': {'question': 'What is a student model in the context of LLM distillation?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('dcd18dd0-03f6-453d-82ab-9710159f8822'), target_run_id=None)],\n",
              "   'execution_time': 5.710846,\n",
              "   'run_id': '3e2532b2-8365-4016-b47f-960dabe039c3',\n",
              "   'output': 'In the context of Large Language Model (LLM) distillation, a \"student model\" refers to a smaller, more efficient model that is trained to replicate the behavior and performance of a larger, more complex model known as the \"teacher model.\" The process of distillation involves transferring knowledge from the teacher model to the student model, with the goal of creating a model that is faster and requires fewer computational resources while maintaining a high level of performance.\\n\\nHere are the key steps involved in the distillation process:\\n\\n1. **Training the Teacher Model**: The teacher model is typically a large, pre-trained model that has been fine-tuned on a specific task or dataset. It serves as the source of knowledge for the student model.\\n\\n2. **Generating Soft Targets**: The teacher model is used to generate \"soft targets\" or probability distributions over the possible outputs for a given input. These soft targets contain more information than hard labels (e.g., the correct class) because they reflect the teacher model\\'s confidence in its predictions.\\n\\n3. **Training the Student Model**: The student model is trained using the soft targets generated by the teacher model. The objective is to minimize the difference between the student model\\'s predictions and the soft targets from the teacher model. This can be done using a loss function such as Kullback-Leibler (KL) divergence.\\n\\n4. **Fine-Tuning**: After the initial distillation, the student model may be further fine-tuned on the original task-specific dataset to improve its performance.\\n\\nThe main advantages of using a student model in LLM distillation are:\\n\\n- **Efficiency**: The student model is smaller and faster, making it more suitable for deployment in resource-constrained environments such as mobile devices or real-time applications.\\n- **Reduced Computational Costs**: Training and inference with the student model require less computational power and memory compared to the teacher model.\\n- **Maintained Performance**: Despite being smaller, the student model can achieve performance levels close to that of the teacher model due to the knowledge transfer during distillation.\\n\\nOverall, LLM distillation with student models is a powerful technique for creating efficient and effective language models that can be used in a wide range of applications.',\n",
              "   'reference': {'must_mention': ['smaller', 'model']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTNe4kWrplB"
      },
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add to our existing conditional edge to obtain the behaviour we desire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajBwLkFVi1N"
      },
      "source": [
        "####üèóÔ∏è Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6rN7feNVn9f"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsXeF6xlaXOZ"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "z_Sq3A9SaV1O"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def tool_call_or_helpful(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    return \"end\"\n",
        "  else:\n",
        "    return \"continue\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "####üèóÔ∏è Activity #4:\n",
        "\n",
        "Please write what is happening in our `tool_call_or_helpful` function!\n",
        "\n",
        "It checks the messages to count how many times it has received the information. If the count exceeds 10, it will end the cycle. It also verifies through a chain if the answer is close to what the user asked."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BhnBW2YVsJO"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    tool_call_or_helpful,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"action\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSI8AOaEVvT-"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "22470df4-9aa7-4751-95b6-d30ce71b17db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_eNsOdrz9FNVlILGLrQkPQ204', 'function': {'arguments': '{\"query\": \"LoRA machine learning\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_MtbuseLmc3s0If1Jz8XQveZn', 'function': {'arguments': '{\"query\": \"Tim Dettmers\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_I40knZJwrotyYWE4DLpn7SBH', 'function': {'arguments': '{\"query\": \"Attention in machine learning\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 171, 'total_tokens': 247}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-b3a520c6-a143-4719-9ab1-308138b85f81-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'LoRA machine learning'}, 'id': 'call_eNsOdrz9FNVlILGLrQkPQ204', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Tim Dettmers'}, 'id': 'call_MtbuseLmc3s0If1Jz8XQveZn', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Attention in machine learning'}, 'id': 'call_I40knZJwrotyYWE4DLpn7SBH', 'type': 'tool_call'}], usage_metadata={'input_tokens': 171, 'output_tokens': 76, 'total_tokens': 247})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='Let\\'s jump on LoRA. Low-Rank Adaptation of LLMs (LoRA) So, in usual fine-tuning, we. Take a pretrained model. Do Transfer Learning over new training data to slightly adjust these pre-trained weights Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning method for large language models. LoRA saves memory by training only low rank perturbations to selected weight matrices. In this work, we compare the performance of LoRA and full finetuning on two target domains, programming and mathematics. We consider both the instruction finetuning ($\\\\\\\\approx$100K prompt-response ... LoRA\\'s approach to decomposing ( Œî W ) into a product of lower rank matrices effectively balances the need to adapt large pre-trained models to new tasks while maintaining computational efficiency. The intrinsic rank concept is key to this balance, ensuring that the essence of the model\\'s learning capability is preserved with significantly ... \"Lora The Tuner\" By Daniel Warfield using MidJourney. All images by the author unless otherwise specified. Fine tuning is the process of tailoring a machine learning model to a specific application, which can be vital in achieving consistent and high quality performance. Feb 18, 2024. Share. Ever since the introduction of BERT in 2019, fine-tuning has been the standard approach to adapt large language models (LLMs) to downstream tasks. This changed with the introduction of LoRA (Hu et al 2021) which showed for the first time that the weight update matrix during fine-tuning can be drastically simplified using ...', name='duckduckgo_search', tool_call_id='call_eNsOdrz9FNVlILGLrQkPQ204'), ToolMessage(content='Tim Dettmers. Video. Tech Moves: AI researcher Yejin Choi leaves Univ. of Washington and Allen Institute for AI. by Todd Bishop & Taylor Soper on August 2, 2024 August 2, 2024 at 11:59 am. Allen School Ph.D. student Tim Dettmers accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required ‚Äî from over 780GB to less than 48GB ‚Äî to finetune a 65B parameter model. With QLoRA, the largest publicly available models can be finetuned on a single ... Tim Dettmers\\'s research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... In the chat quoted in the complaint, researcher Tim Dettmers talks about his back-and-forth with Meta\\'s legal department whether the use of the book files as training data would be \"legally ok ... Tim Dettmers argues that the data (as a batch) can be loaded asynchronously from a hard drive while the GPU is busy processing the previous batch, making loading data from a hard drive not slower ...', name='duckduckgo_search', tool_call_id='call_MtbuseLmc3s0If1Jz8XQveZn'), ToolMessage(content='Learn how attention mechanisms in deep learning enable models to focus on relevant information and improve performance in tasks such as machine translation, image captioning, and speech recognition. Understand the steps and components of attention mechanism architecture and see examples of its applications. The Transformer, since then, has become a popular architecture choice for a variety of tasks. It is capable of capturing long-range dependencies in data making it a powerful tool not only for NLP but also for computer vision, audio, and protein folding. Transformer: the all-powerful in the land of machine learning. Attention mechanism is a fundamental invention in artificial intelligence and machine learning, redefining the capabilities of deep learning models. This mechanism, inspired by the human mental process of selective focus, has emerged as a pillar in a variety of applications, accelerating developments in natural language processing, computer ... An Attention Mechanism in deep learning simulates the human cognitive process of selective focus. It enables neural networks to assign varying degrees of importance to different parts of input ... The introduction of the Transformer model was a significant leap forward for the concept of attention in deep learning. Vaswani et al. described this model in the seminal paper titled \"Attention is All You Need\" in 2017. ... Attention mechanisms represent advancements in machine learning and computer vision, enabling models to prioritize ...', name='duckduckgo_search', tool_call_id='call_I40knZJwrotyYWE4DLpn7SBH')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='### LoRA in Machine Learning\\nLoRA, or Low-Rank Adaptation, is a parameter-efficient fine-tuning method for large language models (LLMs). Traditional fine-tuning involves adjusting the pre-trained weights of a model to adapt it to new tasks, which can be computationally expensive. LoRA simplifies this process by training only low-rank perturbations to selected weight matrices, significantly reducing memory usage while maintaining the model\\'s performance. This approach balances the need to adapt large pre-trained models to new tasks with computational efficiency.\\n\\n### Tim Dettmers\\nTim Dettmers is a researcher focused on making foundation models, such as ChatGPT, more accessible by reducing their resource requirements. His work involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cost-effective deep learning. One of his notable contributions is QLoRA, a method that significantly reduces the GPU memory required to fine-tune large models, making it possible to fine-tune a 65 billion parameter model on a single GPU.\\n\\n### Attention in Machine Learning\\nAttention mechanisms are a fundamental innovation in deep learning, enabling models to focus on relevant parts of the input data. This concept is inspired by the human cognitive process of selective focus. Attention mechanisms have been pivotal in various applications, including natural language processing (NLP), computer vision, and speech recognition. The Transformer model, introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017, is a prime example of how attention mechanisms can capture long-range dependencies in data, making it a powerful tool for a wide range of tasks.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 329, 'prompt_tokens': 1128, 'total_tokens': 1457}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None}, id='run-1ee7331d-ab24-43c9-87c2-52d45e3ce0a0-0', usage_metadata={'input_tokens': 1128, 'output_tokens': 329, 'total_tokens': 1457})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmZPs6lnpsM"
      },
      "source": [
        "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "outputs": [],
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "0593c6f5-0a1d-41f6-960b-f32d101b3ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt engineering is a concept primarily associated with the field of artificial intelligence, particularly in the context of natural language processing (NLP) and large language models like GPT-3. It involves the design and crafting of prompts (input text) to elicit desired responses from AI models. The goal is to optimize the input to get the most accurate, relevant, or useful output from the model.\n",
            "\n",
            "### Key Aspects of Prompt Engineering:\n",
            "1. **Crafting Effective Prompts**: Designing prompts that guide the AI to produce the desired output.\n",
            "2. **Iterative Testing**: Continuously refining prompts based on the responses received.\n",
            "3. **Understanding Model Behavior**: Knowing how different models respond to various types of input.\n",
            "4. **Use Cases**: Applications range from chatbots and virtual assistants to content generation and data analysis.\n",
            "\n",
            "### Emergence of Prompt Engineering:\n",
            "Prompt engineering became more prominent with the advent of large-scale language models like OpenAI's GPT-3, which was released in June 2020. The ability of these models to generate human-like text based on prompts led to a growing interest in how to effectively communicate with them to achieve specific goals.\n",
            "\n",
            "Would you like more detailed information or recent developments in prompt engineering?\n",
            "\n",
            "\n",
            "\n",
            "RAG stands for Retrieval-Augmented Generation. It is a technique in natural language processing (NLP) that combines retrieval-based methods with generative models to improve the quality and relevance of generated text. The key idea is to retrieve relevant documents or pieces of information from a large corpus and use this retrieved information to guide the generation process.\n",
            "\n",
            "RAG was introduced by Facebook AI Research (FAIR) in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published in 2020. The approach leverages both retrieval and generation to handle knowledge-intensive tasks more effectively than traditional methods.\n",
            "\n",
            "Would you like more detailed information or the original paper on RAG?\n",
            "\n",
            "\n",
            "\n",
            "Fine-tuning is a process in machine learning where a pre-trained model is further trained on a new dataset to adapt it to a specific task. This approach leverages the knowledge the model has already acquired during its initial training on a large, general dataset, and refines it to perform well on a more specialized task. Fine-tuning is particularly useful in natural language processing (NLP) and computer vision, where large pre-trained models like BERT, GPT, and ResNet can be adapted to specific applications with relatively small amounts of task-specific data.\n",
            "\n",
            "### Key Steps in Fine-Tuning:\n",
            "1. **Pre-training**: A model is initially trained on a large, general dataset.\n",
            "2. **Transfer Learning**: The pre-trained model is used as a starting point for a new task.\n",
            "3. **Fine-Tuning**: The model is further trained on a smaller, task-specific dataset to adapt it to the new task.\n",
            "\n",
            "### Benefits of Fine-Tuning:\n",
            "- **Efficiency**: Reduces the amount of data and computational resources needed.\n",
            "- **Performance**: Often leads to better performance on the specific task compared to training a model from scratch.\n",
            "- **Speed**: Faster convergence during training.\n",
            "\n",
            "### Historical Context:\n",
            "Fine-tuning became particularly prominent with the advent of deep learning and large-scale pre-trained models. Some key milestones include:\n",
            "\n",
            "- **2014**: The concept of transfer learning and fine-tuning gained traction with the success of models like AlexNet and VGG in computer vision.\n",
            "- **2018**: The release of BERT (Bidirectional Encoder Representations from Transformers) by Google marked a significant breakthrough in NLP, showcasing the power of fine-tuning pre-trained models for various language tasks.\n",
            "- **2019**: OpenAI's GPT-2 demonstrated the effectiveness of fine-tuning in generating coherent and contextually relevant text, further popularizing the approach.\n",
            "\n",
            "To get more specific details on the historical development and key papers, I can look up relevant scientific articles and current events. Would you like me to do that?\n",
            "\n",
            "\n",
            "\n",
            "LLM-based agents, or Large Language Model-based agents, represent a new paradigm in artificial intelligence where large language models are enhanced with the ability to perceive and utilize external resources and tools. This significantly extends their versatility and expertise compared to standalone LLMs.\n",
            "\n",
            "The concept of LLM-based agents has evolved from simplistic direct input-output prompting to more sophisticated autonomous agents capable of complex interactions and tasks. The recent advancements in large language models have been pivotal in shaping this new paradigm.\n",
            "\n",
            "While the exact timeline of when LLM-based agents broke onto the scene isn't specified, it is clear that their development is closely tied to the advancements in large language models, which have seen significant progress in recent years. The integration of agent technology with LLMs marks the beginning of a new era in AI, promising substantial improvements in various applications.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print(messages[\"messages\"][-1].content)\n",
        "  print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
